{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0597c21",
   "metadata": {},
   "source": [
    "# 逻辑回归（Logistic Regression）\n",
    "\n",
    "逻辑回归是一种用于二分类问题的统计学习方法，常用于预测事件的发生概率（例如成功/失败、是/否）。\n",
    "\n",
    "### 与线性回归的区别\n",
    "线性回归：拟合一条直线以预测连续值，关注变量之间的线性关系。\n",
    "\n",
    "逻辑回归：使用非线性函数（如 Sigmoid）预测事件发生的概率，输出值在 0 到 1 之间。\n",
    "\n",
    "### 使用 Sigmoid 函数\n",
    "逻辑回归的核心是 Sigmoid 函数，它将任意实数映射到 (0, 1) 区间：\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "$𝛽_0$：截距项（bias）\n",
    "\n",
    "$𝛽_𝑖$：第$𝑖$个特征的权重系数\n",
    "\n",
    "$𝑥_𝑖$：第$𝑖$个输入特征值\n",
    "\n",
    "## Sigmoid 输出解释\n",
    "\n",
    "| 𝑧 值范围 | Sigmoid 输出 $$\\sigma(z)$$ | 预测含义                     |\n",
    "|-----------------|------------------------|--------------------------|\n",
    "| $$( z \\ll 0 )$$   | 接近 0               | 否定性预测（事件不发生） |\n",
    "| $$( z \\gg 0 )$$  | 接近 1               | 肯定性预测（事件发生）   |\n",
    "| $$( z \\approx 0 )$$ | 接近 0.5          | 模糊边界（不确定）       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e0e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测概率: 0.9089\n",
      "预测类别: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid 函数定义\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 线性组合函数：z = β0 + β1*x1 + β2*x2 + ...\n",
    "def linear_combination(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "# 预测函数：返回概率\n",
    "def predict_proba(X, weights, bias):\n",
    "    z = linear_combination(X, weights, bias)\n",
    "    return sigmoid(z)\n",
    "\n",
    "# 二分类预测：返回 0 或 1\n",
    "def predict(X, weights, bias, threshold=0.5):\n",
    "    proba = predict_proba(X, weights, bias)\n",
    "    return (proba >= threshold).astype(int)\n",
    "\n",
    "# 示例数据\n",
    "X_sample = np.array([2.0, -1.0])        # 特征值\n",
    "weights = np.array([0.8, -0.5])         # 权重\n",
    "bias = 0.2                              # 截距\n",
    "\n",
    "# 执行预测\n",
    "probability = predict_proba(X_sample, weights, bias)\n",
    "label = predict(X_sample, weights, bias)\n",
    "\n",
    "print(f\"预测概率: {probability:.4f}\")\n",
    "print(f\"预测类别: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b244d",
   "metadata": {},
   "source": [
    "## 最大似然估计法（Maximum Likelihood Estimation）\n",
    "\n",
    "在逻辑回归中，我们希望找到一组参数，使得模型对训练数据的预测概率最大。为此，我们定义一个**似然函数**，并通过优化使其最大化。\n",
    "\n",
    "### 数学表达\n",
    "\n",
    "假设训练数据为 $(x_i, y_i)$，其中 $ y_i \\in \\{0, 1\\} $，则逻辑回归的似然函数为：\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^{n} \\sigma(z_i)^{y_i} (1 - \\sigma(z_i))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "我们通常对数化处理，得到对数似然函数：\n",
    "\n",
    "$$\n",
    "\\log L(\\beta) = \\sum_{i=1}^{n} \\left[ y_i \\log \\sigma(z_i) + (1 - y_i) \\log (1 - \\sigma(z_i)) \\right]\n",
    "$$\n",
    "\n",
    "然后使用梯度下降法等优化算法，求出使对数似然函数最大化的参数𝛽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b92247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "估计的参数 β: [-13.50758443   4.9570286 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def log_likelihood(X, y, beta):\n",
    "    z = X @ beta\n",
    "    return np.sum(y * np.log(sigmoid(z)) + (1 - y) * np.log(1 - sigmoid(z)))\n",
    "\n",
    "def logistic_regression(X, y, lr=0.01, max_iter=10000, tol=1e-6):\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros(n)\n",
    "    prev_ll = -np.inf\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        z = X @ beta\n",
    "        gradient = X.T @ (y - sigmoid(z))\n",
    "        beta += lr * gradient\n",
    "\n",
    "        ll = log_likelihood(X, y, beta)\n",
    "        if np.abs(ll - prev_ll) < tol:\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "    return beta\n",
    "\n",
    "# 示例数据\n",
    "X = np.array([\n",
    "    [1, 0.5],\n",
    "    [1, 1.0],\n",
    "    [1, 1.5],\n",
    "    [1, 2.0],\n",
    "    [1, 2.5],\n",
    "    [1, 3.0],\n",
    "    [1, 3.5],\n",
    "    [1, 4.0],\n",
    "    [1, 4.5],\n",
    "    [1, 5.0]\n",
    "])  # 第一列是偏置项\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "beta_hat = logistic_regression(X, y)\n",
    "print(\"估计的参数 β:\", beta_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec628835",
   "metadata": {},
   "source": [
    "### 说明\n",
    "\n",
    "- 使用的是梯度上升法（最大化对数似然）\n",
    "\n",
    "- X 的第一列是常数项（偏置项）\n",
    "\n",
    "- beta_hat 是估计得到的参数向量，可用于预测新样本的概率\n",
    "\n",
    "下面是一个使用 scikit-learn 实现逻辑回归的完整 Python 示例，适用于二分类问题，并包含训练、预测和评估步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1daf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 Accuracy: 0.6666666666666666\n",
      "混淆矩阵 Confusion Matrix:\n",
      " [[1 0]\n",
      " [1 1]]\n",
      "分类报告 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.75      0.75      0.67         3\n",
      "weighted avg       0.83      0.67      0.67         3\n",
      "\n",
      "预测概率 Predicted Probabilities:\n",
      " [[0.15205575 0.84794425]\n",
      " [0.92529562 0.07470438]\n",
      " [0.51048435 0.48951565]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 示例数据：你可以替换为自己的 CSV 文件\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0],\n",
    "    'feature2': [1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "    'target':   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# 特征和目标变量\n",
    "X = data[['feature1', 'feature2']]\n",
    "y = data['target']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 初始化并训练模型\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估\n",
    "print(\"准确率 Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"混淆矩阵 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"分类报告 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 可选：预测概率\n",
    "y_prob = model.predict_proba(X_test)\n",
    "print(\"预测概率 Predicted Probabilities:\\n\", y_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129ea29",
   "metadata": {},
   "source": [
    "### 说明\n",
    "\n",
    "- LogisticRegression() 默认使用 L2 正则化和 lbfgs 求解器\n",
    "\n",
    "- predict() 给出类别预测（0 或 1）\n",
    "\n",
    "- predict_proba() 给出属于每个类别的概率\n",
    "\n",
    "- 可通过 penalty, solver, C, max_iter 等参数进行高级配置"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logistic-ViuxWiqp-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
